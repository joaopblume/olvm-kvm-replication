# Auto-generated DAG for VM: {{ vm_name }}
# Generated: {{ generation_time }}
# Schedule: {{ cron_expression }}

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import ovirtsdk4 as sdk
import ovirtsdk4.types as types
import logging
import os
import time
import json
from checkpoint_manager import CheckpointManager
from time import sleep
from contextlib import contextmanager

# DAG Configuration
default_args = {
    'owner': 'ovirt-backup',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': {{ vm_config.retries | default(0) }},
    'retry_delay': timedelta(minutes={{ vm_config.retry_delay_minutes | default(5) }}),
    'email': ['suporte@oserv.com.br']
}

dag = DAG(
    '{{ dag_id }}',
    default_args=default_args,
    description='Auto-generated backup and restore DAG for VM: {{ vm_name }}',
    schedule='{{ cron_expression }}',
    catchup=False,
    max_active_runs=1,
    tags=['restore', 'backup', 'dinamico', '{{ vm_name | to_python_identifier }}']
)

# VM-specific configuration
VM_NAME = "{{ vm_name }}"
VM_CONFIG = {{ vm_config | python_json }}

# Global configurations
OVIRT_CONFIG = {
    'url': Variable.get('source_ovirt_url'),
    'user': Variable.get('source_ovirt_user'),
    'passwd': Variable.get('source_ovirt_password'),
    'certificate': Variable.get('source_ovirt_cert_path'),
    'backup_dir': Variable.get('backup_directory')
}

@contextmanager
def ovirt_connection():
    """Context manager para garantir fechamento da conexão oVirt"""
    connection = None
    try:
        logging.info("Criando conexão oVirt...")
        connection = sdk.Connection(
            url=OVIRT_CONFIG['url'],
            username=OVIRT_CONFIG['user'],
            password=OVIRT_CONFIG['passwd'],
            ca_file=OVIRT_CONFIG['certificate'],
            log=logging.getLogger(),
            debug=False,
            timeout=30,
            connections=2
        )
        logging.info("Conexão oVirt criada")
        yield connection
    finally:
        if connection:
            logging.info("Fechando conexão oVirt...")
            try:
                connection.close()
                logging.info("Conexão oVirt fechada")
            except Exception as e:
                logging.warning(f"Erro ao fechar conexão: {e}")

def validate_backup_environment(**context):
    """Valida ambiente e determina tipo de backup para VM específica"""
    vm_name = VM_NAME
    
    logging.info(f'Validando ambiente para VM: {vm_name}')
    
    # Verificar espaço em disco
    backup_dir = OVIRT_CONFIG['backup_dir']
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir, exist_ok=True)
    
    # Verificar espaço livre (mínimo 10GB para teste)
    statvfs = os.statvfs(backup_dir)
    free_space = statvfs.f_frsize * statvfs.f_bavail / (1024**3)  # GB
    
    if free_space < 10:
        raise RuntimeError(f'Espaço insuficiente: {free_space:.1f}GB disponível')
    
    # Determinar tipo de backup usando checkpoint manager
    checkpoint_manager = CheckpointManager(backup_dir)
    is_full_backup = checkpoint_manager.should_do_full_backup(vm_name, VM_CONFIG)
    backup_type = 'full' if is_full_backup else 'incremental'
    
    logging.info(f'✓ Validação OK - VM: {vm_name}, Tipo: {backup_type}, Espaço: {free_space:.1f}GB')
    
    return {
        'vm_name': vm_name, 
        'vm_config': VM_CONFIG,
        'backup_type': backup_type
    }

def get_vm_info(**context):
    """Obtém informações da VM incluindo configurações de CPU e memória"""
    validation_info = context['task_instance'].xcom_pull(task_ids='validate_environment')
    vm_name = validation_info['vm_name']
    backup_type = validation_info['backup_type']
    
    with ovirt_connection() as connection:
        # Buscar VM
        vms_service = connection.system_service().vms_service()
        vms = vms_service.list(search=f'name={vm_name}')
        
        if not vms:
            raise NameError(f'VM {vm_name} não encontrada')
        
        vm = vms[0]
        
        # Verificar status - IMAGE_LOCKED será tratado na próxima etapa
        if vm.status not in [types.VmStatus.UP, types.VmStatus.DOWN, types.VmStatus.IMAGE_LOCKED]:
            raise RuntimeError(f'VM {vm.name} em status inválido: {vm.status}')
        
        # Obter discos
        vm_service = vms_service.vm_service(vm.id)
        disk_attachments = vm_service.disk_attachments_service().list()
        
        # Capturar informações dos discos
        disks_info = []
        disks_service = connection.system_service().disks_service()
        
        for attachment in disk_attachments:
            disk = disks_service.disk_service(attachment.disk.id).get()
            disk_info = {
                'disk_id': disk.id,
                'disk_name': disk.name,
                'provisioned_size': disk.provisioned_size,
                'actual_size': disk.actual_size if disk.actual_size else disk.provisioned_size,
                'format': str(disk.format),
                'interface': str(attachment.interface),
                'bootable': attachment.bootable
            }
            disks_info.append(disk_info)
            logging.info(f'  Disco {disk.name}: {disk.provisioned_size / (1024**3):.1f} GB ({disk.format})')
        
        # Capturar configurações de CPU
        cpu_cores = vm.cpu.topology.cores if vm.cpu and vm.cpu.topology else 1
        cpu_sockets = vm.cpu.topology.sockets if vm.cpu and vm.cpu.topology else 1
        cpu_threads = vm.cpu.topology.threads if vm.cpu and vm.cpu.topology else 1
        total_cores = cpu_cores * cpu_sockets * cpu_threads
        
        # Capturar configurações de memória
        memory_mb = vm.memory / (1024**2) if vm.memory else 0
        memory_gb = vm.memory / (1024**3) if vm.memory else 0
        
        disk_count = len(disk_attachments)
        logging.info(f'✓ VM encontrada: {vm.name} (Status: {vm.status})')
        logging.info(f'  CPU: {total_cores} cores ({cpu_sockets}s/{cpu_cores}c/{cpu_threads}t)')
        logging.info(f'  Memória: {memory_gb:.1f} GB')
        logging.info(f'  Discos: {disk_count}')
        
        return {
            **validation_info,
            'vm_id': vm.id,
            'vm_status': str(vm.status),
            'cpu_cores': cpu_cores,
            'cpu_sockets': cpu_sockets,
            'cpu_threads': cpu_threads,
            'total_cores': total_cores,
            'memory_mb': memory_mb,
            'memory_gb': memory_gb,
            'disk_count': disk_count,
            'disks_info': disks_info
        }

def check_and_unlock_vm(**context):
    """Verifica se a VM está bloqueada e desbloqueaia se necessário"""
    vm_info = context['task_instance'].xcom_pull(task_ids='get_vm_info')
    vm_id = vm_info['vm_id']
    vm_name = vm_info['vm_name']
    
    with ovirt_connection() as connection:
        vm_service = connection.system_service().vms_service().vm_service(vm_id)
        vm = vm_service.get()
        
        # Verificar se VM está bloqueada
        if vm.status == types.VmStatus.IMAGE_LOCKED:
            logging.warning(f'VM {vm_name} está bloqueada (IMAGE_LOCKED). Tentando desbloquear...')
            
            try:
                # Verificar se VM pode ser desbloqueada (mais seguro)
                # Primeiro tentar parar snapshots em andamento
                snapshots_service = vm_service.snapshots_service()
                locked_snapshots = [s for s in snapshots_service.list() 
                                  if s.snapshot_status == types.SnapshotStatus.LOCKED]
                
                if locked_snapshots:
                    logging.info(f'Encontrados {len(locked_snapshots)} snapshots bloqueados. Aguardando conclusão...')
                    # Aguardar snapshots terminarem naturalmente
                    timeout = 300  # 5 minutos
                    start_time = time.time()
                    
                    while locked_snapshots and (time.time() - start_time < timeout):
                        time.sleep(15)
                        locked_snapshots = [s for s in snapshots_service.list() 
                                          if s.snapshot_status == types.SnapshotStatus.LOCKED]
                        if locked_snapshots:
                            logging.info(f'Ainda aguardando {len(locked_snapshots)} snapshots terminarem...')
                
                # Tentar desbloquear a VM usando diferentes métodos
                logging.info(f'Tentando desbloquear VM {vm_name}...')
                
                # Método 1: Tentar atualizar status
                try:
                    vm_service.update(types.Vm())  # Update sem parâmetros força refresh
                    time.sleep(5)
                except:
                    pass
                
                # Aguardar desbloqueio
                timeout = 300  # 5 minutos
                start_time = time.time()
                
                while True:
                    vm = vm_service.get()
                    if vm.status != types.VmStatus.IMAGE_LOCKED:
                        logging.info(f'✓ VM {vm_name} desbloqueada. Status atual: {vm.status}')
                        break
                    
                    if time.time() - start_time > timeout:
                        logging.warning(f'Timeout desbloqueando VM {vm_name} após {timeout}s - continuando mesmo assim')
                        break
                    
                    logging.info(f'Aguardando desbloqueio da VM {vm_name}...')
                    time.sleep(10)
                
                # VM foi desbloqueada, finalizar execução do backup
                logging.info(f'VM {vm_name} foi desbloqueada com sucesso. Finalizando execução do backup.')
                return {
                    **vm_info,
                    'vm_was_locked': True,
                    'vm_unlocked': True,
                    'skip_backup': True,
                    'unlock_reason': 'VM estava bloqueada e foi desbloqueada com sucesso'
                }
                
            except Exception as unlock_error:
                logging.error(f'Erro ao desbloquear VM {vm_name}: {unlock_error}')
                raise RuntimeError(f'Falha ao desbloquear VM {vm_name}: {unlock_error}')
        
        # VM não está bloqueada, continuar com o backup
        logging.info(f'✓ VM {vm_name} não está bloqueada. Status: {vm.status}')
        return {
            **vm_info,
            'vm_was_locked': False,
            'vm_unlocked': False,
            'skip_backup': False
        }

def backup_vm_disks(**context):
    """Realiza backup dos discos da VM (full ou incremental)"""
    backup_info = context['task_instance'].xcom_pull(task_ids='check_vm_lock')
    
    # Se snapshot foi pulado, pular backup também
    if backup_info.get('snapshot_skipped', False):
        logging.info(f'Pulando backup de discos - {backup_info.get("snapshot_skip_reason", "Snapshot foi pulado")}')
        return {
            **backup_info,
            'backup_skipped': True,
            'backup_skip_reason': backup_info.get('snapshot_skip_reason', 'Snapshot foi pulado')
        }
    
    vm_name = backup_info['vm_name']
    vm_id = backup_info['vm_id']
    backup_type = backup_info['backup_type']
    
    # Criar diretório de backup
    backup_dir = f"{OVIRT_CONFIG['backup_dir']}/{vm_name}"
    os.makedirs(backup_dir, exist_ok=True)
    
    # Nome do arquivo baseado no tipo
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    backup_filename = f"{vm_name}_{timestamp}_{backup_type}.qcow2"
    backup_file_path = os.path.join(backup_dir, backup_filename)
    
    logging.info(f'Iniciando backup {backup_type} da VM {vm_name}')
    
    # Download actual disk data instead of creating empty files
    logging.info(f'Executando backup {backup_type} usando snapshot: {backup_info.get("snapshot_id", "N/A")}')
    
    # Get checkpoint ID for incremental backups
    checkpoint_manager = CheckpointManager(OVIRT_CONFIG['backup_dir'])
    checkpoint_id = None
    if backup_type == 'incremental':
        checkpoint_id = checkpoint_manager.get_last_checkpoint(vm_name)
        if not checkpoint_id:
            logging.warning(f"No checkpoint found for incremental backup of {vm_name}, performing full backup")
            backup_type = 'full'
    
    # Download VM backup using oVirt Backup API
    backup_result = backup_vm_with_ovirt_api(
        vm_id=vm_id,
        vm_name=vm_name,
        backup_file_path=backup_file_path,
        backup_type=backup_type,
        disks_info=backup_info['disks_info'],
        checkpoint_id=checkpoint_id,
        checkpoint_manager=checkpoint_manager
    )
    
    actual_file_size = backup_result['total_size']
    
    # Usar o primeiro arquivo criado como file path principal (para VMs com um disco)
    # Para VMs com múltiplos discos, todos os arquivos estarão listados nos metadados
    actual_backup_file_path = backup_result.get('backup_files_created', [backup_file_path])[0]
    
    # Checkpoint já foi criado dentro da função backup_vm_with_ovirt_api após sucesso
    checkpoint_record_id = backup_result.get('checkpoint_record_id')
    
    # Criar metadados
    backup_metadata = {
        'vm_name': vm_name,
        'vm_id': vm_id,
        'backup_type': backup_type,
        'backup_date': datetime.now().isoformat(),
        'backup_file_path': actual_backup_file_path,
        'backup_files_created': backup_result.get('backup_files_created', []),
        'file_size': actual_file_size,
        'checkpoint_record_id': checkpoint_record_id,
        'vm_config': {
            'cpu_cores': backup_info['cpu_cores'],
            'cpu_sockets': backup_info['cpu_sockets'], 
            'cpu_threads': backup_info['cpu_threads'],
            'total_cores': backup_info['total_cores'],
            'memory_mb': backup_info['memory_mb'],
            'memory_gb': backup_info['memory_gb']
        },
        'disk_count': backup_info['disk_count'],
        'disks_info': backup_info['disks_info'],
        'status': 'completed',
        'backup_id': backup_result.get('backup_id'),
        'checkpoint_id': backup_result.get('checkpoint_id'),
        'disk_count_backed_up': backup_result.get('disk_count'),
        'backup_description': backup_result.get('backup_description')
    }
    
    # Salvar metadados
    metadata_file = actual_backup_file_path.replace('.qcow2', '.json')
    with open(metadata_file, 'w') as f:
        json.dump(backup_metadata, f, indent=2)
    
    logging.info(f'✓ Backup {backup_type} concluído: {actual_backup_file_path}')
    logging.info(f'✓ Tamanho: {actual_file_size / 1024:.1f} KB')
    logging.info(f'✓ Checkpoint: {checkpoint_record_id}')
    
    return {
        **backup_info,
        'backup_file_path': actual_backup_file_path,
        'backup_files_created': backup_result.get('backup_files_created', []),
        'metadata_file': metadata_file,
        'file_size': actual_file_size,
        'checkpoint_record_id': checkpoint_record_id,
        'backup_completed_at': datetime.now().isoformat()
    }

def cleanup_snapshot(**context):
    """Remove snapshot e aplica retenção"""
    backup_info = context['task_instance'].xcom_pull(task_ids='backup_vm_disks')

    print('Waiting for 60 seconds before cleanup...')
    sleep(60)
    
    # Se backup foi pulado, apenas registrar e finalizar
    if backup_info.get('backup_skipped', False):
        logging.info(f'Finalizando execução - backup foi pulado: {backup_info.get("backup_skip_reason", "Backup foi pulado")}')
        return {
            **backup_info,
            'cleanup_completed': True,
            'cleanup_reason': 'Backup foi pulado devido a VM ter sido desbloqueada'
        }
    
    vm_id = backup_info['vm_id']
    vm_name = backup_info['vm_name']
    vm_config = backup_info['vm_config']
    backup_type = backup_info['backup_type']
    
    with ovirt_connection() as connection:
        
        # Aplicar política de retenção
        checkpoint_manager = CheckpointManager(OVIRT_CONFIG['backup_dir'])
        checkpoint_manager.cleanup_old_checkpoints(vm_name, vm_config)
        
        # Marcar checkpoint como verificado
        checkpoint_manager.mark_checkpoint_verified(backup_info['checkpoint_record_id'])
        
        # Estatísticas
        backup_stats = checkpoint_manager.get_backup_stats(vm_name)
        logging.info(f'Estatísticas de backup para {vm_name}: {backup_stats}')
        
        return {
            **backup_info,
            'cleanup_completed': True,
            'retention_applied': True,
            'backup_stats': backup_stats
        }

def backup_vm_with_ovirt_api(vm_id, vm_name, backup_file_path, backup_type, disks_info, checkpoint_id=None, checkpoint_manager=None):
    """Backup VM using oVirt Backup API (no VM shutdown required)"""
    try:
        from ovirt_imageio import client
    except ImportError:
        logging.error("Missing required module: ovirt_imageio")
        logging.error("Install with: pip install ovirt-imageio-client")
        raise RuntimeError("Missing ovirt_imageio client - cannot download backup data")
    
    with ovirt_connection() as connection:
        backup = None
        backups_service = None
        try:
            # Get VM service
            vm_service = connection.system_service().vms_service().vm_service(vm_id)
            
            # Get VM disks for backup
            disk_attachments = vm_service.disk_attachments_service().list()
            if not disk_attachments:
                raise RuntimeError(f"No disks found for VM {vm_name}")
            
            # Prepare disks for backup
            backup_disks = []
            system_service = connection.system_service()
            
            for attachment in disk_attachments:
                disk_id = attachment.disk.id
                disk = system_service.disks_service().disk_service(disk_id).get()
                backup_disks.append(disk)
                logging.info(f"Adding disk to backup: {disk.name} ({disk.provisioned_size / (1024**3):.1f} GB)")
            
            # Create backup using oVirt Backup API
            backups_service = vm_service.backups_service()
            
            backup_description = f"Backup-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            
            backup = backups_service.add(
                types.Backup(
                    disks=backup_disks,
                    from_checkpoint_id=checkpoint_id if backup_type == 'incremental' and checkpoint_id else None,
                    description=backup_description
                )
            )
            
            logging.info(f"Backup started with ID: {backup.id}")
            logging.info(f"Backup type: {backup_type}")
            if checkpoint_id:
                logging.info(f"From checkpoint: {checkpoint_id}")
            
            # Wait for backup to be ready
            timeout = 600  # 10 minutes
            start_time = time.time()
            
            while backup.phase != types.BackupPhase.READY:
                if time.time() - start_time > timeout:
                    raise RuntimeError(f"Backup timeout after {timeout}s")
                
                time.sleep(5)
                backup = backups_service.backup_service(backup.id).get()
                logging.info(f"Backup phase: {backup.phase}")
            
            logging.info("✓ Backup ready for download")
            
            # Download backup data
            backup_service = backups_service.backup_service(backup.id)
            backup_disks_service = backup_service.disks_service()
            backup_disk_list = backup_disks_service.list()
            
            total_size = 0
            backup_files_created = []
            
            # Create base directory
            backup_dir = os.path.dirname(backup_file_path)
            os.makedirs(backup_dir, exist_ok=True)
            
            # Download each disk
            for i, backup_disk in enumerate(backup_disk_list):
                # Generate filename for each disk
                disk_filename = backup_file_path.replace('.qcow2', f'_disk{i+1}_{backup_disk.id[:8]}.qcow2')
                
                # Check if incremental backup is available
                is_incremental = backup_disk.backup_mode == types.DiskBackupMode.INCREMENTAL
                
                logging.info(f"Downloading disk {i+1}/{len(backup_disk_list)}: {backup_disk.id}")
                logging.info(f"Mode: {'incremental' if is_incremental else 'full'}")
                
                # Create image transfer for backup download
                from helpers import imagetransfer
                
                transfer = imagetransfer.create_transfer(
                    connection,
                    backup_disk,
                    types.ImageTransferDirection.DOWNLOAD,
                    backup=types.Backup(id=backup.id),
                    inactivity_timeout=3600
                )
                
                # Download disk data
                download_url = transfer.transfer_url
                
                # Find backing file for incremental backups
                backing_file = None
                if is_incremental and backup.from_checkpoint_id:
                    backing_file = find_backing_file(backup_dir, backup.from_checkpoint_id, backup_disk.id)
                
                with client.ProgressBar() as pb:
                    client.download(
                        download_url,
                        disk_filename,
                        OVIRT_CONFIG['certificate'],
                        incremental=is_incremental,
                        progress=pb,
                        backing_file=backing_file,
                        backing_format="qcow2" if backing_file else None,
                        secure=False
                    )
                
                # Finalize transfer
                imagetransfer.finalize_transfer(connection, transfer, backup_disk)
                
                disk_size = os.path.getsize(disk_filename)
                total_size += disk_size
                backup_files_created.append(disk_filename)
                logging.info(f"✓ Disk {i+1} downloaded: {disk_size / (1024*1024):.1f} MB")
            
            # Finalize backup
            backup_service.finalize()
            logging.info("Backup finalization started")
            
            # Wait for backup to be fully finalized
            timeout = 300  # 5 minutes
            start_time = time.time()
            
            while True:
                try:
                    backup = backup_service.get()
                    if backup.phase == types.BackupPhase.SUCCEEDED:
                        logging.info("✓ Backup fully finalized and succeeded")
                        break
                    elif backup.phase == types.BackupPhase.FAILED:
                        raise RuntimeError(f"Backup failed during finalization: {backup.phase}")
                    elif time.time() - start_time > timeout:
                        logging.warning(f"Backup finalization timeout after {timeout}s - continuing anyway")
                        break
                    else:
                        logging.info(f"Waiting for backup finalization... Current phase: {backup.phase}")
                        time.sleep(10)
                except Exception as e:
                    if "not found" in str(e).lower():
                        logging.info("✓ Backup finalized (backup object removed)")
                        break
                    else:
                        raise e
            
            # Store checkpoint ID for future incremental backups
            new_checkpoint_id = backup.to_checkpoint_id if backup.to_checkpoint_id else None
            if new_checkpoint_id:
                logging.info(f"New checkpoint created: {new_checkpoint_id}")
            
            # Criar checkpoint local apenas após backup ser finalizado com sucesso
            checkpoint_record_id = None
            if checkpoint_manager:
                actual_backup_file_path = backup_files_created[0] if backup_files_created else backup_file_path
                checkpoint_record_id = checkpoint_manager.create_checkpoint(
                    vm_name=vm_name,
                    vm_id=vm_id,
                    backup_type=backup_type,
                    backup_file_path=actual_backup_file_path,
                    file_size=total_size,
                    checkpoint_id=new_checkpoint_id
                )
                logging.info(f"✓ Checkpoint record created: {checkpoint_record_id}")
            
            return {
                'total_size': total_size,
                'checkpoint_id': new_checkpoint_id,
                'backup_id': backup.id,
                'disk_count': len(backup_disk_list),
                'backup_description': backup_description,
                'backup_files_created': backup_files_created,
                'checkpoint_record_id': checkpoint_record_id
            }
        
        except Exception as backup_error:
            logging.error(f"Failed to backup VM using oVirt API: {backup_error}")
            
            # Check if the error is about a non-existent checkpoint
            error_msg = str(backup_error).lower()
            checkpoint_not_found = (
                "checkpoint" in error_msg and 
                ("doesn't exist" in error_msg or "not found" in error_msg or "invalid" in error_msg)
            )
            
            # If this was an incremental backup that failed, invalidate the checkpoint
            # and force next backup to be full
            if backup_type == 'incremental' and checkpoint_id and checkpoint_manager:
                if checkpoint_not_found:
                    logging.warning(f"Checkpoint {checkpoint_id} doesn't exist in oVirt for {vm_name}")
                else:
                    logging.warning(f"Incremental backup failed for {vm_name}, invalidating checkpoint {checkpoint_id}")
                
                checkpoint_manager.invalidate_checkpoint(vm_name, checkpoint_id)
                checkpoint_manager.force_full_backup_after_failure(vm_name)
                logging.info(f"Next backup for {vm_name} will be forced to full backup")
            
            # Try to finalize backup on error
            try:
                if backup and backups_service:
                    backups_service.backup_service(backup.id).finalize()
                    logging.info("Backup finalizado após erro")
            except Exception as finalize_error:
                logging.warning(f"Erro ao finalizar backup: {finalize_error}")
            raise backup_error

def find_backing_file(backup_dir, checkpoint_id, disk_id):
    """Find backing file for incremental backup"""
    import glob
    pattern = os.path.join(backup_dir, f"*_{checkpoint_id}_{disk_id[:8]}*.qcow2")
    matches = glob.glob(pattern)
    if matches:
        return os.path.basename(matches[0])
    return None

{% if vm_config.auto_restore %}
# ============================================================================
# RESTORE FUNCTIONS (only if auto_restore is enabled)
# ============================================================================

def validate_restore_environment(**context):
    """Validate both source and target environments for restore"""
    try:
        # Get configurations
        source_config = {
            'url': Variable.get('source_ovirt_url'),
            'user': Variable.get('source_ovirt_user'),
            'passwd': Variable.get('source_ovirt_password'),
            'certificate': Variable.get('source_ovirt_cert_path')
        }
        
        target_config = {
            'url': Variable.get('target_ovirt_url'),
            'user': Variable.get('target_ovirt_user'),
            'passwd': Variable.get('target_ovirt_password'),
            'certificate': Variable.get('target_ovirt_cert_path')
        }
        
        # Test connections
        def create_ovirt_connection_from_config(config):
            return sdk.Connection(
                url=config['url'],
                username=config['user'],
                password=config['passwd'],
                ca_file=config['certificate'],
                log=logging.getLogger(),
                debug=False
            )
        
        # Test source connection
        source_conn = create_ovirt_connection_from_config(source_config)
        source_conn.test()
        source_conn.close()
        logging.info('✓ Source environment connectivity OK')
        
        # Test target connection
        target_conn = create_ovirt_connection_from_config(target_config)
        target_conn.test()
        target_conn.close()
        logging.info('✓ Target environment connectivity OK')
        
        # Store configurations for restore tasks
        return {
            'source_config': source_config,
            'target_config': target_config,
            'vm_name': VM_NAME
        }
        
    except Exception as e:
        logging.error(f"Restore environment validation failed: {str(e)}")
        raise

def detect_new_backups(**context):
    """Check if there are new backups available for restore"""
    try:
        restore_info = context['task_instance'].xcom_pull(task_ids='validate_restore_environment')
        vm_name = restore_info['vm_name']
        
        backup_directory = Variable.get('backup_directory')
        checkpoint_manager = CheckpointManager(backup_directory)
        
        # Get last restored checkpoint
        restore_log_file = os.path.join(backup_directory, f'restore_log_{vm_name}.json')
        last_restored_checkpoint = None
        
        if os.path.exists(restore_log_file):
            try:
                with open(restore_log_file, 'r') as f:
                    restore_logs = json.load(f)
                if restore_logs:
                    last_restore = max(restore_logs, key=lambda x: x.get('restore_date', ''))
                    last_restored_checkpoint = last_restore.get('source_checkpoint_id')
                    logging.info(f'Last restored checkpoint: {last_restored_checkpoint}')
            except Exception as e:
                logging.warning(f'Error reading restore log: {e}')
        
        # Get current backup chain
        backup_chain = checkpoint_manager.get_checkpoint_chain(vm_name)
        
        if not backup_chain:
            logging.info(f'No backups found for {vm_name}')
            return {
                **restore_info,
                'has_new_backups': False,
                'skip_restore': True,
                'skip_reason': f'No backups found for {vm_name}'
            }
        
        latest_checkpoint = backup_chain[-1]
        latest_checkpoint_id = latest_checkpoint['id']
        
        # Check if there's a newer backup
        if last_restored_checkpoint != latest_checkpoint_id:
            logging.info(f'✓ New backup detected for {vm_name}: {latest_checkpoint_id}')
            logging.info(f'  Date: {latest_checkpoint["backup_date"]}')
            logging.info(f'  Type: {latest_checkpoint["backup_type"]}')
            
            return {
                **restore_info,
                'has_new_backups': True,
                'skip_restore': False,
                'backup_chain': backup_chain,
                'latest_checkpoint': latest_checkpoint,
                'vm_config': VM_CONFIG
            }
        else:
            logging.info(f'{vm_name} is up to date (checkpoint: {latest_checkpoint_id})')
            return {
                **restore_info,
                'has_new_backups': False,
                'skip_restore': True,
                'skip_reason': f'{vm_name} is up to date'
            }
            
    except Exception as e:
        logging.error(f"Error checking for new backups: {str(e)}")
        raise

def process_vm_restores(**context):
    """Process VM restores to DR environment"""
    try:
        backup_info = context['task_instance'].xcom_pull(task_ids='detect_new_backups')
        
        # Skip if no new backups
        if backup_info.get('skip_restore', False):
            logging.info(f'Skipping restore: {backup_info.get("skip_reason", "No reason provided")}')
            return {
                **backup_info,
                'restore_completed': False,
                'restore_skipped': True
            }
        
        vm_name = backup_info['vm_name']
        backup_chain = backup_info['backup_chain']
        latest_checkpoint = backup_info['latest_checkpoint']
        target_config = backup_info['target_config']
        
        backup_directory = Variable.get('backup_directory')
        
        # Validate backup chain
        checkpoint_manager = CheckpointManager(backup_directory)
        if not checkpoint_manager.validate_checkpoint_chain(vm_name):
            raise RuntimeError(f'Invalid checkpoint chain for VM {vm_name}')
        
        # Check backup files exist
        missing_files = []
        for checkpoint in backup_chain:
            if not os.path.exists(checkpoint['backup_file_path']):
                missing_files.append(checkpoint['backup_file_path'])
        
        if missing_files:
            raise RuntimeError(f'Backup files not found: {missing_files}')
        
        # Get backup metadata
        metadata_file = latest_checkpoint['backup_file_path'].replace('.qcow2', '.json')
        if not os.path.exists(metadata_file):
            raise RuntimeError(f'Metadata file not found: {metadata_file}')
        
        with open(metadata_file, 'r') as f:
            backup_metadata = json.load(f)
        
        # Connect to DR environment
        def create_ovirt_connection_from_config(config):
            return sdk.Connection(
                url=config['url'],
                username=config['user'],
                password=config['passwd'],
                ca_file=config['certificate'],
                log=logging.getLogger(),
                debug=False
            )
        
        connection = create_ovirt_connection_from_config(target_config)
        
        try:
            # Restore VM to DR
            restored_vm_id = restore_vm_to_dr(
                connection=connection,
                vm_name=vm_name,
                backup_metadata=backup_metadata,
                backup_chain=backup_chain,
                vm_config=VM_CONFIG
            )
            
            # Record restore
            restore_record = {
                'vm_name': vm_name,
                'restored_vm_id': restored_vm_id,
                'restore_date': datetime.now().isoformat(),
                'source_checkpoint_id': latest_checkpoint['id'],
                'restore_action': 'auto_restore',
                'status': 'completed',
                'auto_restore': True
            }
            
            # Save restore log
            restore_log_file = os.path.join(backup_directory, f'restore_log_{vm_name}.json')
            restore_logs = []
            
            if os.path.exists(restore_log_file):
                with open(restore_log_file, 'r') as f:
                    restore_logs = json.load(f)
            
            restore_logs.append(restore_record)
            
            with open(restore_log_file, 'w') as f:
                json.dump(restore_logs, f, indent=2)
            
            logging.info(f'✓ VM {vm_name} successfully restored to DR')
            
            return {
                **backup_info,
                'restore_completed': True,
                'restore_skipped': False,
                'restored_vm_id': restored_vm_id,
                'restore_record': restore_record
            }
            
        finally:
            connection.close()
            
    except Exception as e:
        logging.error(f"VM restore failed: {str(e)}")
        raise

def restore_vm_to_dr(connection, vm_name, backup_metadata, backup_chain, vm_config):
    """Restore VM to DR environment using latest backup - COMPLETE IMPLEMENTATION"""
    logging.info(f'Starting restore of VM {vm_name} to DR environment')
    latest_checkpoint = backup_chain[-1]
    
    # Validate backup chain  
    backup_directory = Variable.get('backup_directory')
    checkpoint_manager = CheckpointManager(backup_directory)
    if not checkpoint_manager.validate_checkpoint_chain(vm_name):
        raise RuntimeError(f'Invalid checkpoint chain for VM {vm_name}')
    
    # Check backup files exist
    missing_files = []
    for checkpoint in backup_chain:
        if not os.path.exists(checkpoint['backup_file_path']):
            missing_files.append(checkpoint['backup_file_path'])
    
    if missing_files:
        raise RuntimeError(f'Backup files not found: {missing_files}')
    
    # Check if VM exists in DR - but we'll ALWAYS create a new VM with _clone suffix
    vms_service = connection.system_service().vms_service()
    existing_vms = vms_service.list(search=f'name={vm_name}')
    
    # Check for old cloned VMs and temporarily rename if exists
    old_cloned_vms = vms_service.list(search=f'name={vm_name}_clone')
    old_clone_temp_name = None
    old_clone_id = None
    
    if old_cloned_vms:
        # Temporarily rename the old clone to avoid conflicts
        old_clone_vm = old_cloned_vms[0]
        old_clone_id = old_clone_vm.id
        old_clone_temp_name = f"{vm_name}_clone_temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        old_clone_service = vms_service.vm_service(old_clone_id)
        old_clone_service.update(types.Vm(name=old_clone_temp_name))
        logging.info(f'Temporarily renamed old clone to: {old_clone_temp_name}')
    
    restore_action = 'create'  # ALWAYS create new VM with _clone suffix
    existing_vm_id = existing_vms[0].id if existing_vms else None
    
    logging.info(f'Original VM exists: {len(existing_vms) > 0}')
    logging.info(f'Old cloned VM exists: {len(old_cloned_vms) > 0}')
    logging.info(f'Will create new VM with _clone suffix - original VM preserved')
    
    # Create new VM in DR
    restored_vm_id = create_vm_in_dr_complete(
        connection=connection,
        vm_name=vm_name,
        backup_metadata=backup_metadata,  
        restore_action=restore_action,
        existing_vm_id=existing_vm_id
    )
    
    try:
        # Restore VM disks
        restore_vm_disks_complete(connection, restored_vm_id, backup_chain, backup_metadata, vm_config)
        
        # Finalize restore
        old_vm_backup_id = finalize_vm_restore_complete(
            connection=connection,
            vm_name=vm_name,
            restored_vm_id=restored_vm_id,
            restore_action=restore_action,
            existing_vm_id=existing_vm_id,
            old_clone_id=old_clone_id,
            old_clone_temp_name=old_clone_temp_name,
            latest_checkpoint=latest_checkpoint
        )
        
        # Cleanup old cloned VM if exists (NEVER cleanup original VM)
        if old_vm_backup_id:
            cleanup_old_vm_complete(connection, old_vm_backup_id, old_clone_temp_name or f"{vm_name}_clone_old")
        
        return restored_vm_id
        
    except Exception as disk_error:
        # Clean up failed VM
        logging.error(f'Disk restore failed for {vm_name}: {disk_error}')
        if restored_vm_id:
            cleanup_failed_vm_complete(connection, restored_vm_id, f"{vm_name}_restore_failed")
        
        # Restore old clone name if it was renamed
        if old_clone_id and old_clone_temp_name:
            try:
                old_clone_service = vms_service.vm_service(old_clone_id)
                old_clone_service.update(types.Vm(name=f"{vm_name}_clone"))
                logging.info(f'Restored old clone name back to: {vm_name}_clone')
            except Exception as restore_error:
                logging.error(f'Failed to restore old clone name: {restore_error}')
        
        raise disk_error

def create_vm_in_dr_complete(connection, vm_name, backup_metadata, restore_action, existing_vm_id=None):
    """Create VM in DR environment - ALWAYS with _clone suffix"""
    temp_vm_name = f"{vm_name}_clone"
    
    # Get cluster and template
    clusters_service = connection.system_service().clusters_service()
    clusters = clusters_service.list()
    if not clusters:
        raise RuntimeError('No cluster found in DR environment')
    
    default_cluster = clusters[0]
    
    templates_service = connection.system_service().templates_service()
    blank_template = templates_service.list(search='name=Blank')[0]
    
    # VM configurations
    vm_config = backup_metadata['vm_config']
    
    # Get restore settings from variables
    restore_settings = {
        'chipset_type': Variable.get('restore_settings', deserialize_json=True, default_var={}).get('chipset_type', 'q35'),
        'firmware_type': Variable.get('restore_settings', deserialize_json=True, default_var={}).get('firmware_type', 'bios')
    }
    
    # Create VM
    vms_service = connection.system_service().vms_service()
    vm_create = types.Vm(
        name=temp_vm_name,
        cluster=types.Cluster(id=default_cluster.id),
        template=types.Template(id=blank_template.id),
        memory=int(vm_config['memory_mb'] * 1024 * 1024),
        cpu=types.Cpu(
            topology=types.CpuTopology(
                cores=vm_config['cpu_cores'],
                sockets=vm_config['cpu_sockets'],
                threads=vm_config['cpu_threads']
            )
        ),
        bios=types.Bios(
            type=types.BiosType.Q35_SEA_BIOS if restore_settings['chipset_type'] == 'q35' and restore_settings['firmware_type'] == 'bios' else types.BiosType.I440FX_SEA_BIOS
        ),
        description=f"Auto-restored from backup on {datetime.now().isoformat()}"
    )
    
    new_vm = vms_service.add(vm_create)
    logging.info(f'VM created in DR: {new_vm.name} (ID: {new_vm.id})')
    
    # Wait for VM to be available
    vm_service = vms_service.vm_service(new_vm.id)
    timeout = 300
    start_time = time.time()
    
    while True:
        vm = vm_service.get()
        if vm.status == types.VmStatus.DOWN:
            break
        elif time.time() - start_time > timeout:
            raise RuntimeError(f'Timeout waiting for VM to be available')
        time.sleep(5)
    
    return new_vm.id

def merge_backup_chain_complete(backup_chain, temp_dir):
    """Merge incremental backup chain into a single optimized disk"""
    import subprocess
    import tempfile
    
    if len(backup_chain) == 1:
        # Single file, no merge needed
        return backup_chain[0]['backup_file_path']
    
    # Sort chain by level to ensure proper order
    sorted_chain = sorted(backup_chain, key=lambda x: x['chain_level'])
    
    logging.info(f'Merging {len(sorted_chain)} files in backup chain')
    
    logging.info(f'Chain: {" → ".join([os.path.basename(cp["backup_file_path"]) for cp in sorted_chain])}')
    
    # We need to manually rebuild the backing chain for qemu-img to work
    # Create temporary copies with correct backing file relationships
    temp_files = []
    
    try:
        # Start with the full backup (base)
        base_file = sorted_chain[0]['backup_file_path']
        current_base = os.path.join(temp_dir, f"base_{int(time.time())}.qcow2")
        
        # Copy the full backup as our base
        logging.info(f'Copying base file: {os.path.basename(base_file)}')
        subprocess.run(['cp', base_file, current_base], check=True)
        temp_files.append(current_base)
        
        # Apply each incremental on top, building the chain
        for i, checkpoint in enumerate(sorted_chain[1:], 1):
            incremental_file = checkpoint['backup_file_path']
            temp_incremental = os.path.join(temp_dir, f"incremental_{i}_{int(time.time())}.qcow2")
            
            logging.info(f'Processing incremental {i}: {os.path.basename(incremental_file)}')
            
            # Copy incremental file
            subprocess.run(['cp', incremental_file, temp_incremental], check=True)
            temp_files.append(temp_incremental)
            
            # Rebase the incremental to point to our current base
            cmd = [
                'qemu-img', 'rebase',
                '-u',  # Unsafe mode (don't check backing file format)
                '-b', current_base,  # New backing file
                '-F', 'qcow2',  # Backing file format
                temp_incremental
            ]
            logging.info(f'Rebasing: qemu-img rebase -u -b {os.path.basename(current_base)} {os.path.basename(temp_incremental)}')
            subprocess.run(cmd, check=True)
            
            # Now commit this incremental into the base to create a new merged base
            new_base = os.path.join(temp_dir, f"merged_{i}_{int(time.time())}.qcow2")
            
            # Convert the incremental + backing into a single standalone file
            cmd = [
                'qemu-img', 'convert',
                '-O', 'qcow2',
                temp_incremental,  # This will follow backing chain
                new_base
            ]
            logging.info(f'Merging: qemu-img convert {os.path.basename(temp_incremental)} {os.path.basename(new_base)}')
            subprocess.run(cmd, check=True)
            
            temp_files.append(new_base)
            current_base = new_base
        
        # Skip final compression - the merged file is already optimized
        # Final compression can actually increase file size in many cases
        optimized_file = current_base
        
        # Verify the final file
        cmd = ['qemu-img', 'info', optimized_file]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        logging.info(f'Final file info: {result.stdout}')
        
        # Get final file size for reporting
        final_size = os.path.getsize(optimized_file)
        original_total = sum(os.path.getsize(cp['backup_file_path']) for cp in backup_chain)
        
        logging.info(f'✓ Chain merged successfully (no additional compression)')
        logging.info(f'  Original total: {original_total/(1024**2):.1f} MB')
        logging.info(f'  Merged result: {final_size/(1024**2):.1f} MB')
        if original_total > 0:
            logging.info(f'  Space difference: {((original_total - final_size) / original_total * 100):.1f}%')
        
        return optimized_file
        
    except Exception as e:
        logging.error(f'Error during chain merge: {e}')
        # Cleanup temp files on error
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
        raise

def restore_vm_disks_complete(connection, vm_id, backup_chain, backup_metadata, vm_config=None):
    """Restore VM disks using oVirt ImageIO with optimized chain merging"""
    logging.info(f'Restoring disks for VM {vm_id}')
    
    # Import required modules
    try:
        from ovirt_imageio import client
        from helpers import imagetransfer
        import subprocess
        import tempfile
    except ImportError:
        logging.error("Missing required modules: ovirt_imageio, helpers.imagetransfer or subprocess")
        raise RuntimeError("Missing required modules for disk restore")
    
    # Get available storage domains
    storage_domains_service = connection.system_service().storage_domains_service()
    storage_domains = storage_domains_service.list()
    
    if not storage_domains:
        raise RuntimeError('No storage domain found in DR environment')
    
    # Find data storage domain
    data_storage = None
    for sd in storage_domains:
        if sd.type == types.StorageDomainType.DATA:
            data_storage = sd
            logging.info(f'✓ Using storage domain: {data_storage.name}')
            break
    
    if not data_storage:
        raise RuntimeError('No active data storage domain found')
    
    # Group backup chain by disk (in case of multiple disks per VM)
    disks_by_name = {}
    for checkpoint in backup_chain:
        # Extract disk identifier from filename
        filename = os.path.basename(checkpoint['backup_file_path'])
        # Assuming format: VM_date_type_diskN_id.qcow2
        parts = filename.split('_')
        if len(parts) >= 4:
            disk_key = parts[-2]  # disk1, disk2, etc.
        else:
            disk_key = 'disk1'  # fallback
        
        if disk_key not in disks_by_name:
            disks_by_name[disk_key] = []
        disks_by_name[disk_key].append(checkpoint)
    
    logging.info(f'Found {len(disks_by_name)} disk(s) to restore: {list(disks_by_name.keys())}')
    
    # Create temporary directory for merging in backup directory (not /tmp to avoid space issues)
    backup_directory = Variable.get('backup_directory')
    temp_merge_dir = os.path.join(backup_directory, 'temp_restore', f'{vm_id}_{int(time.time())}')
    os.makedirs(temp_merge_dir, exist_ok=True)
    
    try:
        logging.info(f'Using temp directory: {temp_merge_dir}')
        
        disks_service = connection.system_service().disks_service()
        vm_service = connection.system_service().vms_service().vm_service(vm_id)
        disk_ids = []
        
        target_config = {
            'certificate': Variable.get('target_ovirt_cert_path')
        }
        
        for disk_num, (disk_name, disk_chain) in enumerate(disks_by_name.items()):
            logging.info(f'Processing {disk_name} with {len(disk_chain)} files')
            
            # Merge backup chain into single optimized file
            merged_file = merge_backup_chain_complete(disk_chain, temp_merge_dir)
            
            # Get info from merged file
            try:
                out = subprocess.check_output(['qemu-img', 'info', '--output', 'json', merged_file]).decode('utf-8')
                import json
                disk_info = json.loads(out)
                initial_size = disk_info.get('actual-size', 0)
                virtual_size = disk_info.get('virtual-size', 1024 * 1024 * 1024)  # 1GB fallback
            except Exception as e:
                logging.warning(f'Error getting qcow2 info: {e}, using default values')
                initial_size = os.path.getsize(merged_file)
                virtual_size = 128 * 1024 * 1024 * 1024  # 128GB fallback
            
            logging.info(f'  Virtual size: {virtual_size / (1024**3):.1f} GB')
            logging.info(f'  Actual size: {initial_size / (1024**2):.1f} MB')
            
            # Create optimized disk
            upload_disk = disks_service.add(
                types.Disk(
                    name=f'{disk_name}_merged_restored',
                    format=types.DiskFormat.COW,
                    initial_size=initial_size,
                    backup=types.DiskBackup.INCREMENTAL,
                    content_type=types.DiskContentType.DATA,
                    provisioned_size=virtual_size,
                    storage_domains=[
                        types.StorageDomain(
                            name=data_storage.name
                        )
                    ]
                )
            )
            
            # Wait for disk to be OK
            disk_service = disks_service.disk_service(upload_disk.id)
            timeout = 300
            start_time = time.time()
            
            while True:
                disk = disk_service.get()
                if disk.status == types.DiskStatus.OK:
                    logging.info("✓ Disk created and ready for upload")
                    break
                elif time.time() - start_time > timeout:
                    raise RuntimeError(f'Timeout creating disk')
                else:
                    logging.info(f"Waiting for disk... Status: {disk.status}")
                    time.sleep(10)
            
            # Find host and create image transfer
            host = imagetransfer.find_host(connection, data_storage.name)
            
            transfer = imagetransfer.create_transfer(
                connection,
                upload_disk,
                types.ImageTransferDirection.UPLOAD,
                host=host,
                inactivity_timeout=3600,  # 1 hour for large uploads
            )
            
            try:
                logging.info(f"Starting upload of optimized {disk_name}...")
                upload_url = transfer.transfer_url
                
                # Upload merged file
                with client.ProgressBar() as pb:
                    client.upload(
                        merged_file,
                        upload_url,
                        target_config['certificate'],
                        buffer_size=client.BUFFER_SIZE,
                    )
                
                logging.info(f"✓ Upload of {disk_name} completed")
                disk_ids.append(upload_disk.id)
                
            except Exception as upload_error:
                logging.error(f"Upload error: {upload_error}")
                imagetransfer.cancel_transfer(connection, transfer)
                raise upload_error
            
            # Finalize transfer
            imagetransfer.finalize_transfer(connection, transfer, disk)
            logging.info(f"✓ Disk {disk_name} restored and optimized")
        
        # Attach disks to VM
        logging.info("Attaching disks to VM...")
        disk_attachments_service = vm_service.disk_attachments_service()
        
        for disk_num, disk_id in enumerate(disk_ids):
            disk_attachment = types.DiskAttachment(
                disk=types.Disk(id=disk_id),
                interface=types.DiskInterface.SATA,
                bootable=(disk_num == 0),  # First disk is bootable
                active=True,
            )
            
            disk_attachments_service.add(disk_attachment)
            logging.info(f"✓ Disk {disk_num + 1} attached to VM")
        
        logging.info(f"✓ All {len(disk_ids)} disks restored, optimized and attached")
    
    finally:
        # Cleanup temporary directory
        try:
            import shutil
            if os.path.exists(temp_merge_dir):
                shutil.rmtree(temp_merge_dir)
                logging.info(f"✓ Temporary directory cleaned up: {temp_merge_dir}")
        except Exception as cleanup_error:
            logging.warning(f"Error cleaning up temp directory: {cleanup_error}")

def finalize_vm_restore_complete(connection, vm_name, restored_vm_id, restore_action, existing_vm_id, old_clone_id, old_clone_temp_name, latest_checkpoint):
    """Finalize restore process - NEVER touch the original VM"""
    vms_service = connection.system_service().vms_service()
    
    # ALWAYS ensure restored VM has _clone suffix and original VM is NEVER touched
    restored_vm_service = vms_service.vm_service(restored_vm_id)
    new_vm_name = f"{vm_name}_clone"
    restored_vm_service.update(types.Vm(name=new_vm_name))
    
    logging.info(f'Original VM preserved with name: {vm_name}')
    logging.info(f'Restored VM created with name: {new_vm_name}')
    if old_clone_temp_name:
        logging.info(f'Old clone will be removed: {old_clone_temp_name}')
    
    # Return old clone ID for cleanup if exists (NOT the original VM)
    old_vm_backup_id = old_clone_id if old_clone_id else None
    
    # Record restore
    restore_record = {
        'vm_name': vm_name,
        'restored_vm_id': restored_vm_id,
        'restore_date': datetime.now().isoformat(),
        'source_checkpoint_id': latest_checkpoint['id'],
        'restore_action': restore_action,
        'old_vm_backup_id': existing_vm_id,
        'status': 'completed',
        'auto_restore': True
    }
    
    # Save restore log
    backup_directory = Variable.get('backup_directory')
    restore_log_file = os.path.join(backup_directory, f'restore_log_{vm_name}.json')
    restore_logs = []
    
    if os.path.exists(restore_log_file):
        with open(restore_log_file, 'r') as f:
            restore_logs = json.load(f)
    
    restore_logs.append(restore_record)
    
    with open(restore_log_file, 'w') as f:
        json.dump(restore_logs, f, indent=2)
    
    logging.info(f'Restore finalized for {vm_name} - checkpoint: {latest_checkpoint["id"]}')
    
    return old_vm_backup_id

def cleanup_failed_vm_complete(connection, vm_id, vm_name):
    """Remove VM that failed during restore"""
    try:
        logging.info(f'Removing failed VM: {vm_name} (ID: {vm_id})')
        vms_service = connection.system_service().vms_service()
        vm_service = vms_service.vm_service(vm_id)
        
        # Stop VM if running
        try:
            vm = vm_service.get()
            if vm.status == types.VmStatus.UP:
                logging.info(f'Stopping VM before removal: {vm_name}')
                vm_service.stop()
                time.sleep(60)
        except Exception as e:
            logging.warning(f'Error stopping VM {vm_name}: {e}')
        
        # Remove VM
        vm_service.remove()
        logging.info(f'✓ VM removed: {vm_name}')
        
    except Exception as e:
        logging.error(f'Error removing failed VM {vm_name}: {e}')

def cleanup_old_vm_complete(connection, vm_id, vm_name):
    """Remove old VM after successful restore"""
    try:
        logging.info(f'Removing old VM: {vm_name} (ID: {vm_id})')
        vms_service = connection.system_service().vms_service()
        vm_service = vms_service.vm_service(vm_id)
        
        sleep(10)
        # Stop VM if running
        try:
            vm = vm_service.get()
            if vm.status == types.VmStatus.UP:
                logging.info(f'Stopping old VM before removal: {vm_name}')
                vm_service.stop()
                time.sleep(60)
        except Exception as e:
            logging.warning(f'Error stopping old VM {vm_name}: {e}')
        
        # Remove VM
        vm_service.remove()
        logging.info(f'✓ Old VM removed: {vm_name}')
        
    except Exception as e:
        logging.error(f'Error removing old VM {vm_name}: {e}')

{% endif %}

# Define backup tasks
validate_task = PythonOperator(
    task_id='validate_environment',
    python_callable=validate_backup_environment,
    dag=dag
)

vm_info_task = PythonOperator(
    task_id='get_vm_info',
    python_callable=get_vm_info,
    dag=dag
)

lock_check_task = PythonOperator(
    task_id='check_vm_lock',
    python_callable=check_and_unlock_vm,
    dag=dag
)

backup_task = PythonOperator(
    task_id='backup_vm_disks',
    python_callable=backup_vm_disks,
    dag=dag
)

cleanup_task = PythonOperator(
    task_id='cleanup_snapshot',
    python_callable=cleanup_snapshot,
    retries=2,
    retry_delay=timedelta(minutes=2),
    dag=dag
)

{% if vm_config.auto_restore %}
# Define restore tasks (only if auto_restore is enabled)
validate_restore_task = PythonOperator(
    task_id='validate_restore_environment',
    python_callable=validate_restore_environment,
    dag=dag
)

detect_backups_task = PythonOperator(
    task_id='detect_new_backups',
    python_callable=detect_new_backups,
    dag=dag
)

process_restores_task = PythonOperator(
    task_id='process_vm_restores',
    python_callable=process_vm_restores,
    dag=dag
)

# Task dependencies with restore
validate_task >> vm_info_task >> lock_check_task >> backup_task >> cleanup_task >> validate_restore_task >> detect_backups_task >> process_restores_task
{% else %}
# Task dependencies without restore
validate_task >> vm_info_task >> lock_check_task >> backup_task >> cleanup_task
{% endif %}